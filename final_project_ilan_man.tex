\documentclass{article}
\usepackage{blindtext}
\usepackage[margin=0.45in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{stackengine}
\usepackage[pdftex]{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{verbatim} 
\usepackage{setspace}
\graphicspath{ {/Users/ilanman/Duke/STA561/project} }
\usepackage{stackengine}
\setlength{\parindent}{0pt}
 
\title{Beyond SVD\\ \large{STA561 (Probabilistic Machine Learning) Final Project}}
\author{\textbf{Ilan Man}\\
Department of Statistical Science\\
Duke University\\
\texttt{ilanman@gmail.com}}

\begin{document}
\maketitle

\section{Introduction}

Singular value decomposition plays an important role in high-quality statistical computations and in schemes for data compression and least squares solutions. Many modern computational algorithms are based on singular value computations because the problem of computing the eigenvalues of a general matrix is well-conditioned. However, traditional SVD on a large matrix can be very computationally expensive.
\\

This paper introduces the concept random sampling to approximate SVD calculations. We outline two randomized SVD algorithms and an important lemma which provides a theoretical error bound for mapping a set of points in high dimensional space onto a lower dimensional subspace. We finish with some empirical results showing the speed up gained from using randomized methods vs. classical SVD.

\section{Singular Value Decomposition}

\subsection{Matrix Factorization}

Before reviewing SVD, we motivate by discussing matrix factorizations. Matrix factorizations underpin many matrix manipulations commonly found in machine learning and statistical algorithms. Formally, matrix factorization is an operation on a matrix $A$ such that it is factored into two smaller matrices, $R$ and $Q$:
\begin{equation}
\underset{n\times m}{\mathrm{A}} =  \underset{n\times k}{R} \times \underset{k\times m}{Q^{T}}
\end{equation}

Where $k \leq \text{min}(n,m)$. Since $RQ^{T}$ may not be exactly equal to $A$, this results in a residual term, $\varepsilon$ that we seek to minimize. Mathematically, we want $||A - RQ^{T}||_{F} \leq \varepsilon$. This norm, known as the Frobenius norm, is a common matrix norm, where $||A||^{2}_{F} = \text{trace}(A^{T}A)$.
\\

One approach is to use partial-QR decomposition. This is an iterative algorithm where $A$ factors into $QR$ such that $R$ is \textit{upper triangular}. This makes it much easier to multiply against, and reduces the computational expense. At iteration $t$, we get:
$$A^{(t)} = Q^{(t)}R^{(t)} + E^{(t)}$$
where $E^{(t)}$ is an error residual that satisfies $||E^{(t)}||_{F}\leq \varepsilon$. The more steps that we take in this algorithm the closer $||E^{(t)}||_{F}$ get to the theoretical $\varepsilon$. More iterations also results in higher operational complexity. 
%While QR decomposition is very commonly used, outside of matrix factorization, its application is somewhat limited. Instead, we seek a more useful, wide ranging approach, singular value decomposition.
\\

There are multiple ways to factorize a matrix, each providing an advantage and disadvantage \cite{factorize}. This paper focuses on singular value decomposition. 

\subsection{SVD algorithm}

We begin by stating the \textbf{SVD Theorem}: \textit{If A is an m $\times$ n matrix, then A has a singular value decomposition.}
\\

The SVD of a real $m \times n$ matrix $A$ is a factorization of the form $A = U \Sigma V^{T}$ where:

\begin{itemize}
\item $U$ is an $m$ x $k$ orthogonal matrix of left singular vectors of $A$
\item $\Sigma$ is an $k$ x $k$ diagonal matrix of ordered singular values of $A$, i.e. $\sigma_{1} \geq \sigma_{2} \geq \hdots \geq \sigma_{k}$. 
\item $V^{T}$ is an $k$ x $n$ orthogonal matrix of right singular vectors of $A$
\end{itemize}

$$\Sigma = \left[ {\begin{array}{*{10}ccc}
    \sigma_{1} & & & \\
    & \sigma_{2} & & \\
    & & \ddots & \\
    & & & \sigma_{k} 
\end{array} } \right]$$
\\

Computing the SVD is a fundamental linear algebra objective with a wide range of applications. Two of the most common are low-rank approximations and solutions to least squares. We briefly discuss these two topics.

\subsubsection{Low Rank Approximation}

For high-dimensional data, we often want to simplify the data so that traditional machine learning and statistical techniques can be applied. However, crucial information intrinsic in the data should not be removed under this simplification. A widely used method for this purpose is to approximate the data matrix, $A$, with a matrix of lower rank.
\\

Formally, we seek:
\begin{equation}
\underset{B}{\text{min}}||A-B||_{F}: \text{rank}(B) = k, \text{where } k\leq \text{rank}(A)
\end{equation}

Note that the minimization objective has a very similar structure to that above for matrix factorizations.

\subsubsection{Least Squares Approximation}

Solving equations of the form $A\textbf{x}=\textbf{b}$ is an extremely common task. We want to minimize:
\begin{equation}
||A\textbf{x}-\textbf{b}||_{2}
\end{equation}

The best minimizer is $\textbf{x} = (A^{T}A)^{-1}A^{T}\textbf{b}$, the so-called normal equations. To solve this, one might form the matrix $K = A^{T}A$ and use a Cholesky or QR decomposition. Note that the most expensive part of this is computing $A^{T}A$, making this prohibitive in some cases. Another approach is using QR factorization, typically via the Householder algorithm. This approach, while more numerically stable, is roughly twice as expensive as solving the normal equations \cite{lsq}.
\\

SVD is the most numerically stable solution of these methods, especially when $A$ is rank-deficient, because it exposes eigenvalues. However it can be much more computationally expensive.\\

It's application to various problems motivates why technicians are focused on coming up with faster SVD routines.

\subsubsection{Comments}

Note the $m \times n$ matrix $A$ can be decomposed into a sum of $r$ rank-one matrices:
\begin{equation}
A = \sum_{i=1}^{r}\sigma_{i}u_{i}v^{T}_{i}
\end{equation}

Moreover, by the Eckart-Young theorem, the best 2-norm or Frobenius-norm approximation of rank $k\text{ }(0 \leq k \leq n)$ to A is given by $$A_{k} = \sum_{i=1}^{k}\sigma_{i}u_{i}v^{T}_{i}$$
And in fact, $$||A-A_{k}||_{F} = \sigma_{k+1}$$

As mentioned above, SVD exposes the eigenvalues of the matrix $A$. Eigenvalues are found by solving the characteristic polynomial, which is inherently an iterative process, because the roots cannot be found in polynomial time. Specifically, the computational cost of performing SVD is $O(nm\times \text{min}(n,m))$. This becomes very expensive for large, dense matrices. 
\\

One solution is to compute a truncated-SVD. This is equivalent to selecting the top $k$ singular values from $\Sigma$ and associated singular vectors.
\\

Another solution to the compute a partial-QR decomposition on the large matrix $A$. Then on the resulting factor, compute an SVD. This approach scales with $O(kmn)$.
\\

Note that each of these approaches requires random access to memory, i.e. the matrix must be stored in RAM. If the matrix is stored out of core, the methods mentioned above can become very slow. This motivates new techniques for factorizing lage $A$.
\\

\section{Randomized SVD}

\subsection{Motivation}

As mentioned earlier, traditional SVD algorithms can be computationally expensive for large scale problems. SVD also assumes that the matrix can fit in RAM. This is not well suited for missing or noisy data, matrices that are very large or distributed/parallel computation.
\\

To combat these and other items, randomized matrix algorithms were developed. A low-rank approximation can be obtained by randomly sampling columns of A according to a probability distribution that depends on the Euclidean norms of the columns. We describe the algorithm below.

\subsection{RVSD Algorithm}

The following is a generic formulation of the randomized SVD, along with the corresponding Python code, to show how trivial implementation is:
\\

Given: $m \times n$ matrix $A$ and a desired rank $k$:

\begin{enumerate}
\item Draw an $n \times k$ Gaussian random matrix $\Omega$ with elements $\omega_{ij} \stackrel{iid}{\sim} N(0,1)$
\begin{verbatim}
  Omega = np.random.randn(m, k)
\end{verbatim}
\item Compute $H = A \times \Omega$
\begin{verbatim}
  H = np.dot(A,Omega)
\end{verbatim}
\item Construct $Q \in R^{m \times k}$ with columns forming an orthonormal basis for the range of $H$
\begin{verbatim}
  Q, R = np.qr(H)
\end{verbatim}
\item Form the $k \times n$ matrix, $B_{k} = Q^{T}A$.
\begin{verbatim}
  B = np.dot(Q.T,A)
\end{verbatim}
\item Return $B_{k}$
\end{enumerate}

We are interested in comparing the error $\varepsilon_{k} = ||A - B_{k}||_{F}$ to the theoretically minimum error $\sigma_{k+1}$.

\subsubsection{Comments}

RSVD methods allow us to reorganize the calculations to exploit matrix arithmetic and distributed computer networks. As a result, these methods are well suited for parallel implementation. The bottleneck in these calculations is $A\Omega$ which ends up being very parallelizable.
\\

This, \textbf{fixed-rank} algorithm, assumes that we know the target rank, $k$, of our matrix. In some applications, this makes sense, such as genomic identification, where practioners have an idea of the effective population size. In other applications, a low rank $k$ is not known in advance (or perhaps may not exist). The number of columns, $l$, that the algorithm needs in order to minimize $\varepsilon$ is slightly larger than $k$. We call this discrepency the oversampling parameter $p$. You can think of this as a fudge parameter, usually set at 5 or 10. Therefore we write $k = l + p$ in the algorithm above.
\\

To set a reasonable value for $l$, we can run a search algorithm that finds some $l$ corresponding to the smallest error. This problem formulation is known as the \textbf{fixed-precision} problem, because we are solving for a precision in $\varepsilon$, rather than a rank.
\\

\subsubsection{Numerical vs. Statistical perspective}

One of the criticisms of the fixed-precision problem, as stated by numerical analysts, is that the error they seek to minimize is the training, or in-sample error. This doesn't provide useful information about the generalization, or out-of-sample error, which is the key metric we typically want to minimize. More useful would be to find some lower rank $l$ such that we can minimize the out-of-sample error. This way of thinking is motivated by the idea that our data matrix, $A$, is produced by some data-generating process. We seek to find a way to approximate this process in a lower dimensional space. This is the way a statistician would go about solving the problem. However, we'll set aside this discussion for another paper.

\subsection{Johnson-Lindenstrauss}

Upon seeing the RSVD algorithm, a natural question to ask is how are we confident that we can map our original matrix $A$ onto a smaller matrix $B_{k}$ in a way that doesn't disrupt the structure of our data? We want to project onto a lower-dimensional subspace, while reducing all distances by the same common factor, leaving the relative ordering of distances unchanged. Furthermore, we want to do this in polynomial time. The following lemma provides the theoretical basis for this:
\\

\textbf{Johnson-Lindenstrauss Lemma}:
\\ 
\text{Let $\varepsilon \in (0, \frac{1}{2})$. Let $Q \in R^{d}$ be a set of $n$ points and $k = \frac{20\: log\: n}{\varepsilon^{2}}$. There exists a map $f : \mathbb{R}^d \rightarrow \mathbb{R}^k$ such that for all $u, v \in Q:$}

$$(1-\varepsilon)||u-v||^{2}\leq||f(u)-f(v)||^{2}\leq(1+\varepsilon)||u-v||^{2}$$

Two key observations:

\begin{itemize}
\item For some dimension $k < n$, with high probability, there exists a mapping that does not change the pairwise distance between any two points by more than a small factor $(1 \pm \varepsilon)$
\item $k$ is randomly found, in polynomial time
\end{itemize}

This result allows us to apply random projections in the RSVD algorithm and not be concerned with maintaining the relationship of our original data. A proof of the lemma can be found here \cite{JL}.
\\

Ultimately, a projection from $\mathbb{R}^d$ to $\mathbb{R}^k$ takes $O(kd)$ time. For large, $d$, even this could be slow. As a result there have been recent advances in speeding up this process; one such advance is the Fast JL-transform \cite{FastJL}.

\subsection{Power Scheme}

Section 3.2 outlined the generic RSVD algorithm. One enhancement is to include power iterations. This involves repeated multiplication of the original matrix $A$ and computing the QR factorization of the result. We can think of the problem as follows:

$$A^{k} = U\Sigma^{k} V^{T}$$

Recall that we can write $A$ as $\sum_{i=1}^{n}\lambda_{i}u_{i}v_{i}^{t}$. Calculating $A^{k}$ is analogous to raising $\lambda_{i}$ to the $k$th power. We can imagine $\lambda_{i} > 1$ being magnifide, and $\lambda_{i} < 1 \rightarrow 0$. Visually:

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{eigendecay}
\caption{Separating Eigenvalues with power iterations}
\end{figure}

We can compute $A^{2}$, $A^{3}$, etc... and perform QR factorization on the resulting matrix which tends to be more accurate than performing it on $A$, because our important eigenvalues are amplified. This saves a lot of computational expense.
\\

An enhancement to the algorithm is as follows (replacing $k$ with $l + p$, the oversampling parameter):

\begin{enumerate}
\item Draw an $n \times (l+p)$ Gaussian random matrix $\Omega$ with elements $\omega_{ij} \stackrel{iid}{\sim} N(0,1)$
\begin{verbatim}
  Omega = np.random.randn(m, l + p)
\end{verbatim}
\item For $i: 1, ..., q$
\begin{verbatim}
  for i in range(iters):
\end{verbatim}
  \begin{enumerate}
    \item Compute $H = A \times \Omega$
    \item Compute $H = (AA^{T})^{q}H$, by iterating $q$ times
    \begin{verbatim}
    H = np.dot(A,np.dot(A.T,H))
    \end{verbatim}
  \end{enumerate}
\item Compute a $QR$ factorization of $H$. $Q$ is an $n \times (l+p)$ matrix whose columns form an orthonormal basis for $H$.
\begin{verbatim}
  Q, R = np.qr(Omega)
\end{verbatim}
\item Form the $(l + p) \times n$ matrix, $B_{k} = Q^{T}A$.
\begin{verbatim}
  B = np.dot(Q.T,A)
\end{verbatim}
\item Return $B_{k}$
\end{enumerate}

This algorithm requires $2q + 1$ times as many matrix–vector multiplies as the one without power iterations, but is far more accurate in situations where the singular values of $A$ decay slowly \cite{martinsson}. 

\subsection{Empirical Results}

Below we present empirical results comparing the RSVD algorithm outlined in this paper to Python's built-in SVD library, in \verb|numpy|. We tested using a dense matrix of random Gaussian elements:

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[scale=0.5]{power_time}
\caption{RSVD computes much faster}
\label{fig:minipage1}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[scale=0.5]{power_error}
\caption{Using power iterations improves error}
\label{fig:minipage2}
\end{minipage}
\end{figure}

The speed-up gained from using Randomized algorithms is orders of magnitude faster than the out of the box svd method. And using a modest power iteration, the error is minimal. The benefits of using Randomized methods is readily apparent.

\section{Conclusion}

Classical SVD is a very useful and ubiquotous algorithm for solving least squares and low-rank approximations. There are downsides, however, specifically when the matrix being factorized is very large. If we know that there exists a latent rank in a smaller subspace, we can use randomized algorithms to efficiently approximate a lower rank. We discussed a randomized SVD algorithm, and showed how trivial it is to code in Python. Then we highlighted an important theoretical finding that provides error bounds on this low dimensional projection. Finally, we showed how power iterations increases our accuracy many fold.\\

There are many more Randomized techniques, including Adaptive Randomized SVD, which I hope to explore in further research. 

{\footnotesize
\begin{thebibliography}{5} 
\bibitem{factorize} http://www.dcsc.tudelft.nl/bdeschutter/pub/rep/02012.pdf
\bibitem{lsq} http://math.uchicago.edu/~may/REU2012/REUPapers/Lee.pdf
\bibitem{JL} http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf
\bibitem{FastJL} https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf
\bibitem{martinsson} http://arxiv.org/pdf/0909.4061v2.pdf
\end{thebibliography}
}
\end{document}




















